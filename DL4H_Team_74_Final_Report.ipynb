{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgTKlm-o7LP9"
      },
      "source": [
        "# **CS 598 Deep Learning for Healthcare Final Project**\n",
        "\n",
        "# **Domain Knowledge Guided Deep Learning with Electronic Health Records**\n",
        "\n",
        "\n",
        "\n",
        "Team ID: 74\n",
        "\n",
        "Paper ID: 69\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Byunggeun BK Park (bpark14@illinois.edu)\n",
        "\n",
        "Spencer Arbour (sarbour2@illinois.edu)\n",
        "\n",
        "Yun Gao (yungao3@illinois.edu)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Please find our public repository at: https://drive.google.com/drive/folders/1SOuU9TfnHdI9R6mxhpTNvlaZ61zbgn5p?usp=sharing\n",
        "\n",
        "Public Github repository: https://github.com/BK1147/CS598_DLH_Final_Project.git\n",
        "\n",
        "Youtube or GDrive:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "In the realm of healthcare, the utilization of Electronic Health Records (EHRs) for predictive analytics stands as a beacon of opportunity for enhancing patient care and resource allocation. However, the complexity inherent in EHR data, characterized by its high dimensionality, sparsity, and irregularity, poses formidable challenges to accurate prediction of clinical outcomes. Addressing these challenges is paramount for unlocking the full potential of EHRs in revolutionizing healthcare practices.\n",
        "\n",
        "This paper presents a pioneering solution to the intricate problem of clinical risk prediction using EHRs: the Domain Knowledge Guided Recurrent Neural Networks (DG-RNN). By seamlessly integrating rich medical knowledge into the predictive modeling process, DG-RNN transcends the limitations of conventional approaches, offering unprecedented accuracy, interpretability, and adaptability in clinical risk assessment.\n",
        "\n",
        "The cornerstone of DG-RNN lies in its innovative architecture, which harnesses the power of deep learning alongside domain-specific medical insights. Through a Graph-Based Attention Mechanism, the model dynamically incorporates complex relationships between medical events, treatments, and outcomes, enriching the predictive process with invaluable contextual information. Furthermore, a Global Max-Pooling Layer enhances interpretability by spotlighting the most influential medical events contributing to each prediction, thereby bridging the gap between data-driven predictions and clinical intuition.\n",
        "\n",
        "Key to the efficacy of DG-RNN is its adept handling of the idiosyncrasies of EHR data. By encoding temporal irregularities and leveraging advanced attention mechanisms, the model navigates the sparsity and irregularity of patient records with finesse, ensuring robust performance across diverse datasets and prediction tasks.\n",
        "\n",
        "Through rigorous evaluation on real-world EHR datasets, particularly focusing on heart failure risk prediction, DG-RNN showcases its superiority over traditional methods and contemporary deep learning approaches. Not only does it excel in accuracy, but it also stands out for its interpretability, aligning seamlessly with clinical insights and facilitating informed decision-making by healthcare practitioners.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility\n",
        "\n",
        "The original paper introduces the Domain Knowledge Guided Recurrent Neural Networks (DG-RNN), a model that incorporates complex medical knowledge directly into the predictive modeling process using a recurrent neural network architecture. This model is equipped with a graph-based attention mechanism and a global max-pooling layer, designed to enhance predictive accuracy and interpretability for clinical outcomes. The study investigates four variations of the RNN model:\n",
        "\n",
        "1. RNN without additional layers\n",
        "2. RNN with reversed input layer\n",
        "3. RNN with global max-pooling\n",
        "4. RNN with global max-pooling + reversed input layer\n",
        "\n",
        "These models are evaluated against traditional machine learning methods such as Random Forest, Logistic Regression, and Support Vector Machine, as well as advanced deep learning models including GRU, LSTM, and RETAIN. Additionally, for the ablation study, we include a naive bi-directional RNN model with an additional reversed input layer to compare with other models.\n",
        "\n",
        "Due to the incomplete availability of the original implementation files and resources, we plan to recreate the high-level concepts and methodologies presented in the original paper. Our experimental setup will use the MIMIC-IV dataset to validate the effectiveness and reproducibility of the proposed methods.\n",
        "\n",
        "#### Hypotheses to be Tested\n",
        "\n",
        "- **Hypothesis 1**: The basic RNN model will outperform traditional machine learning methods (Random Forest, Logistic Regression, Support Vector Machine) in predicting patient risk. This is expected due to the RNN's ability to effectively capture temporal dependencies and sequential patterns in EHR data, which are critical for accurate clinical risk prediction.\n",
        "\n",
        "- **Hypothesis 2**: An RNN model enhanced with reversed input layer and a global max-pooling layer will exhibit superior performance compared to a basic RNN configuration. The reversed input layer is anticipated to improve the model’s ability to dynamically incorporate relevant medical knowledge, while the global max-pooling layer should help in capturing the most salient features from the input sequences, thus enhancing predictive accuracy and model generalization.\n",
        "\n",
        "By testing these hypotheses, we aim to assess the reproducibility of the original paper's claims and explore the practical benefits of advanced RNN architectures in clinical risk prediction using MIMIC-IV data. Through this process, we also plan to identify any potential challenges and limitations in replicating the study’s results, which will contribute valuable insights for future research in the field of healthcare analytics.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P61bNweuMbw8"
      },
      "source": [
        "## Environment\n",
        "\n",
        "Python version\n",
        "*  Python 3.11\n",
        "    \n",
        "Dependencies/packages needed\n",
        "*  Refer to requirements.txt file\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "\n",
        "### Data download instruction\n",
        "*  An encrypted version of the patient used for running our model have been include `enc_hfs_dataset_pkl`.\n",
        "*  A secret key will be included as a separate download/note in our submission. Save this file to the root directory for this project as `secret.key`.\n",
        "*  Alternatively, create a text file using Vim (or similar) to copy/paste the key provided, saving the file as `secret.key` in the root directory.\n",
        "\n",
        "### Data description\n",
        "*  MIMIC IV data tables (version 2.2) are used as a basis for patient data analysis. The data was collected from PhysioNet. Our dataset was pulled from 'hosp' patient EHRs, and were filtered to only contain ICD-9 codes.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Pre-processing of the MIMIC IV dataset has been moved into `Data_Preprocess.ipynb`. Please reference that notebook for questions regarding filtering of the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GAAIqkV9Mvja"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# set seed\n",
        "seed = 24\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "# ensure data path exists in directory\n",
        "DATA_PATH = 'data/'\n",
        "DATA_PATH_PICKLE = 'data/'\n",
        "os.makedirs(os.path.dirname(DATA_PATH), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleans up file structure from previous runs (if any)\n",
        "dir = DATA_PATH\n",
        "\n",
        "for file in os.listdir(dir):\n",
        "    file_path = os.path.join(dir, file)\n",
        "    os.remove(file_path)\n",
        "\n",
        "if os.path.isfile('enc_diagnoses'):\n",
        "    os.remove('enc_diagnoses')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Unencrypt `enc_hfs_dataset_pkl` and store in `data` directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from cryptography.fernet import Fernet\n",
        "\n",
        "with open('secret.key', 'rb') as mykey:\n",
        "    key = mykey.read()\n",
        "\n",
        "f = Fernet(key)\n",
        "\n",
        "with open('enc_hfs_dataset_pkl', 'rb') as encrypted_file:\n",
        "    encrypted = encrypted_file.read()\n",
        "\n",
        "decrypted = f.decrypt(encrypted)\n",
        "\n",
        "with open(os.path.join(DATA_PATH,'hfs_dataset_pkl.zip'), 'wb') as decrypted_file:\n",
        "    decrypted_file.write(decrypted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unzip pickle files and store in /data directory\n",
        "import shutil\n",
        "import pathlib\n",
        "\n",
        "shutil.unpack_archive(os.path.join(DATA_PATH,'hfs_dataset_pkl.zip'), os.path.join(DATA_PATH,'hfs_pickle_files/'), format='zip')\n",
        "\n",
        "dir = os.path.join(DATA_PATH, 'hfs_pickle_files/')\n",
        "\n",
        "for file in os.listdir(dir):\n",
        "    file_path = os.path.join(dir, file)\n",
        "    shutil.move(file_path, DATA_PATH)\n",
        "\n",
        "pathlib.Path.unlink(os.path.join(DATA_PATH, 'hfs_dataset_pkl.zip'))\n",
        "pathlib.Path.rmdir(os.path.join(DATA_PATH, 'hfs_pickle_files'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iaP-KPzpDnYE"
      },
      "outputs": [],
      "source": [
        "# comment for the each data\n",
        "pids = pickle.load(open(os.path.join(DATA_PATH_PICKLE,'pids.pickle'), 'rb'))\n",
        "vids = pickle.load(open(os.path.join(DATA_PATH_PICKLE,'vids.pickle'), 'rb'))\n",
        "hfs = pickle.load(open(os.path.join(DATA_PATH_PICKLE,'hfs.pickle'), 'rb'))\n",
        "seqs = pickle.load(open(os.path.join(DATA_PATH_PICKLE,'seqs.pickle'), 'rb'))\n",
        "types = pickle.load(open(os.path.join(DATA_PATH_PICKLE,'types.pickle'), 'rb'))\n",
        "rtypes = pickle.load(open(os.path.join(DATA_PATH_PICKLE,'rtypes.pickle'), 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCPNgzPahm6Q",
        "outputId": "e3956ade-013e-42e4-a53e-544619939d1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36975"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(pids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNSdhPkuaK9K"
      },
      "source": [
        "### Pre-processing: Filter 'patient_df' and 'diagnoses_df' into list data used for RNN\n",
        "\n",
        "where\n",
        "* pids: contains the patient ids\n",
        "* vids: contains a list of visit ids for each patient\n",
        "* hfs: contains the heart failure label (0: normal, 1: heart failure) for each patient\n",
        "* seqs: contains a list of visit (in ICD9 codes) for each patient\n",
        "* types: contains the map from ICD9 codes to ICD-9 labels\n",
        "* rtypes: contains the map from ICD9 labels to ICD9 codes\n",
        "\n",
        "Read in `d_icd_diagnoses.csv` and `diagnoses.csv` as Pandas DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('enc_d_icd_diagnoses', 'rb') as encrypted_file:\n",
        "    encrypted = encrypted_file.read()\n",
        "\n",
        "decrypted = f.decrypt(encrypted)\n",
        "\n",
        "with open(os.path.join(DATA_PATH,'d_icd_diagnoses.csv'), 'wb') as decrypted_file:\n",
        "    decrypted_file.write(decrypted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZNf6Sp2Yl-D",
        "outputId": "a8b8b792-6be1-4fcf-b7f0-c33a5b495fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  icd_code  icd_version                             long_title\n",
            "0     0010            9         Cholera due to vibrio cholerae\n",
            "1     0011            9  Cholera due to vibrio cholerae el tor\n",
            "2     0019            9                   Cholera, unspecified\n",
            "3     0020            9                          Typhoid fever\n",
            "4     0021            9                    Paratyphoid fever A\n"
          ]
        }
      ],
      "source": [
        "d_icd_diagnoses_df = pd.read_csv(DATA_PATH + '/d_icd_diagnoses.csv', sep=',', header='infer')\n",
        "print(d_icd_diagnoses_df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1sZIXoSLOnC0W6D5DxQFXmsJozBO_gcGX\n",
            "From (redirected): https://drive.google.com/uc?id=1sZIXoSLOnC0W6D5DxQFXmsJozBO_gcGX&confirm=t&uuid=039efb4a-07ec-440a-af10-1c375ce831c7\n",
            "To: /Users/spenny/CS598_Final_Proj/CS598_DLH_Final_Project/enc_diagnoses\n",
            "100%|██████████| 180M/180M [00:19<00:00, 9.37MB/s] \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'enc_diagnoses'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/file/d/1sZIXoSLOnC0W6D5DxQFXmsJozBO_gcGX/view?usp=share_link\"\n",
        "output = 'enc_diagnoses'\n",
        "\n",
        "gdown.download(url=url, output=output, fuzzy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('enc_diagnoses', 'rb') as encrypted_file:\n",
        "    encrypted = encrypted_file.read()\n",
        "\n",
        "decrypted = f.decrypt(encrypted)\n",
        "\n",
        "with open(os.path.join(DATA_PATH,'diagnoses_icd.csv'), 'wb') as decrypted_file:\n",
        "    decrypted_file.write(decrypted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVevf6drZWhN",
        "outputId": "451bee96-2078-4b66-bb36-0206eab52b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   subject_id   hadm_id  seq_num icd_code  icd_version\n",
            "0    10000032  22595853        1     5723            9\n",
            "1    10000032  22595853        2    78959            9\n",
            "2    10000032  22595853        3     5715            9\n",
            "3    10000032  22595853        4    07070            9\n",
            "4    10000032  22595853        5      496            9\n"
          ]
        }
      ],
      "source": [
        "diagnoses_df = pd.read_csv(DATA_PATH + '/diagnoses_icd.csv', sep=',', header='infer')\n",
        "print(diagnoses_df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2YXZRv2h-J9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The MIMIC-IV dataset is a large and comprehensive collection of medical records, containing over 120,000 unique patient identifiers ('pid'). While this extensive dataset provides a wealth of information, training machine learning models on such a vast dataset can lead to overtraining and overfitting issues. Overfitting occurs when a model learns the training data too well, including its noise and outliers, thereby reducing its ability to generalize to new, unseen data.\n",
        "\n",
        "To mitigate these challenges and improve the model's performance, we have decided to train our models on a smaller, carefully selected subset of the MIMIC-IV dataset. By focusing on a more manageable dataset, we aim to create models that are more robust, generalize better to new data, and ultimately yield more reliable predictions in clinical settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w10A2Aqi2HPh"
      },
      "outputs": [],
      "source": [
        "total_length = len(pids)\n",
        "random_indices = random.sample(range(total_length), 1000)\n",
        "\n",
        "# Select the 2000 elements using the random indices\n",
        "pids = [pids[i] for i in random_indices]\n",
        "vids = [vids[i] for i in random_indices]\n",
        "hfs = [hfs[i] for i in random_indices]\n",
        "seqs = [seqs[i] for i in random_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRk7-KR2fW8I"
      },
      "source": [
        "Pre-Processing for Machine Learning model: Random Forest (RF), Logistic Regression(LR), Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rOHNOh8a3xOQ"
      },
      "outputs": [],
      "source": [
        "d_icd_diagnoses_df = pd.read_csv(DATA_PATH + '/d_icd_diagnoses.csv', sep=',', header='infer')\n",
        "diagnoses_df = pd.read_csv(DATA_PATH + '/diagnoses_icd.csv', sep=',', header='infer')\n",
        "\n",
        "merged_df = pd.merge(diagnoses_df,d_icd_diagnoses_df, on=['icd_code', 'icd_version'], how='left')\n",
        "merged_df['hfs'] = merged_df['long_title'].str.contains(\"heart failure\").astype(int)\n",
        "merged_df = merged_df[merged_df['icd_version'] == 9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QEB7CHUUg1fk"
      },
      "outputs": [],
      "source": [
        "grouped_df = merged_df.groupby('subject_id').agg({\n",
        "    'seq_num': 'sum',\n",
        "    'hfs': 'max'\n",
        "}).reset_index()\n",
        "\n",
        "grouped_df = grouped_df[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##  Model\n",
        "\n",
        "Model architecture\n",
        "\n",
        "---\n",
        "\n",
        "**RNN without additional layers**\n",
        "\n",
        "Layers | Configuration | Activation Function | Output Dimension (batch, feature)\n",
        "--- | --- | --- | ---\n",
        "Embedding | num_embeddings=num_codes, embedding_dim=128 | - | (batch_size, seq_len, embedding_dim)\n",
        "GRU (RNN) | input_size=128, hidden_size=hidden_size | - | (batch_size, seq_len, hidden_size)\n",
        "Linear (fully connected) | input_size=hidden_size , output_size=1 | - | (batch_size, 1)\n",
        "Sigmoid | - | - | (batch_size, 1)\n",
        "\n",
        "**RNN with reversed input layer**\n",
        "\n",
        "Layers | Configuration | Activation Function | Output Dimension (batch, feature)\n",
        "--- | --- | --- | ---\n",
        "Embedding | num_embeddings=num_codes, embedding_dim=128 | - | (batch_size, seq_len, embedding_dim)\n",
        "GRU (RNN) | input_size=128, hidden_size=hidden_size | - | (batch_size, seq_len, hidden_size)\n",
        "GRU (Reverse RNN) | input_size=128, hidden_size=hidden_size | - | (batch_size, seq_len, hidden_size)\n",
        "Linear (fully connected) | input_size=hidden_size * 2, output_size=1 | - | (batch_size, 1)\n",
        "Sigmoid | - | - | (batch_size, 1)\n",
        "\n",
        "**RNN with global max-pooling layer**\n",
        "\n",
        "Layers | Configuration | Activation Function | Output Dimension (batch, feature)\n",
        "--- | --- | --- | ---\n",
        "Embedding | num_embeddings=num_codes, embedding_dim=128 | - | (batch_size, seq_len, embedding_dim)\n",
        "GRU (RNN) | input_size=128, hidden_size=hidden_size | - | (batch_size, seq_len, hidden_size)\n",
        "AdaptiveMaxPool1d | output_size=hidden_size | - | (batch_size, hidden_size)\n",
        "Linear (fully connected) | input_size=hidden_size, output_size=1 | - | (batch_size, 1)\n",
        "Sigmoid | - | - | (batch_size, 1)\n",
        "\n",
        "**RNN with global max-pooling layer + reversed input layer**\n",
        "\n",
        "Layers | Configuration | Activation Function | Output Dimension (batch, feature)\n",
        "--- | --- | --- | ---\n",
        "Embedding | num_embeddings=num_codes, embedding_dim=128 | - | (batch_size, seq_len, embedding_dim)\n",
        "GRU (RNN) | input_size=128, hidden_size=hidden_size | - | (batch_size, seq_len, hidden_size)\n",
        "GRU (Reverse RNN) | input_size=128, hidden_size=hidden_size | - | (batch_size, seq_len, hidden_size)\n",
        "AdaptiveMaxPool1d | output_size=hidden_size | - | (batch_size, hidden_size)\n",
        "Linear (fully connected) | input_size=hidden_size * 2, output_size=1 | - | (batch_size, 1)\n",
        "Sigmoid | - | - | (batch_size, 1)\n",
        "\n",
        "\n",
        "Training objectives:\n",
        "*   Loss Function: Binary Cross-Entropy Loss (assuming the model is for binary classification since the output is passed through a sigmoid activation).\n",
        "*   Optimizer: Adam optimizer with a learning rate of 1e-3.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Disclamer\n",
        "\n",
        "Disclaimer: We utilized the basic RNN structure code from Homework 3 for the RNN model. Using this basic structure as a foundation, we will combine it with a naive bi-directional mechanism and a global max-pooling layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iJ2fvsicKFyW"
      },
      "outputs": [],
      "source": [
        "# Custom dataset\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, seqs, hfs):\n",
        "    self.seqs = seqs\n",
        "    self.hfs = hfs\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.seqs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.seqs[index], self.hfs[index]\n",
        "\n",
        "dataset = CustomDataset(seqs, hfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_bx4P_G9KFkR"
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "    sequences, labels = zip(*data)\n",
        "    y = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    num_patients = len(sequences)\n",
        "    num_visits = [len(patient) for patient in sequences]\n",
        "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
        "\n",
        "    max_num_visits = max(num_visits)\n",
        "    max_num_codes = max(num_codes)\n",
        "\n",
        "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "    for i_patient, patient in enumerate(sequences):\n",
        "        for j_visit, visit in enumerate(patient):\n",
        "            padded_visit = torch.tensor(visit + [0] * (max_num_codes - len(visit)), dtype=torch.long)\n",
        "            x[i_patient, j_visit, :] = padded_visit\n",
        "            masks[i_patient, j_visit, :] = padded_visit != 0\n",
        "\n",
        "    for i_patient, patient in enumerate(sequences):\n",
        "        idx_all_real_visits = torch.sum(x[i_patient, :, :], dim=1) != 0\n",
        "        idx_padded_visits = torch.sum(x[i_patient, :, :], dim=1) == 0\n",
        "        reversed_real_visits = torch.flip(x[i_patient, idx_all_real_visits, :], dims=(0,))\n",
        "        rev_x[i_patient, :, :] = torch.cat((reversed_real_visits, x[i_patient, idx_padded_visits, :]), dim=0)\n",
        "        rev_masks[i_patient, :, :] = rev_x[i_patient, :, :] != 0\n",
        "\n",
        "    return x, masks, rev_x, rev_masks, y\n",
        "\n",
        "def load_data(train_dataset, val_dataset, collate_fn):\n",
        "    batch_size = 32\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def sum_embeddings_with_mask(x, masks):\n",
        "\n",
        "    masked_embeddings = x * masks.unsqueeze(-1)\n",
        "    sum_embeddings = masked_embeddings.sum(dim=2)\n",
        "\n",
        "    return sum_embeddings\n",
        "\n",
        "def get_last_visit(hidden_states, masks):\n",
        "\n",
        "    true_visit_length = torch.sum(masks, dim=1)\n",
        "    idx_last_visit = true_visit_length[:,0] - 1\n",
        "    last_hidden_state = hidden_states[torch.arange(hidden_states.size(0)), idx_last_visit, :]\n",
        "\n",
        "    return last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2lFISxgQ8Uj",
        "outputId": "5104a60f-1b7f-4008-a9ae-f75bfba8b1da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of train dataset: 800\n",
            "Length of train with HF: 298\n",
            "Length of train with survive patients: 502\n",
            "Length of val dataset: 200\n",
            "Length of val with HF: 77\n",
            "Length of val with survive patients: 723\n"
          ]
        }
      ],
      "source": [
        "split = int(len(dataset)*0.8)\n",
        "\n",
        "lengths = [split, len(dataset) - split]\n",
        "train_dataset, val_dataset = random_split(dataset, lengths)\n",
        "\n",
        "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)\n",
        "print(\"Length of train dataset:\", len(train_dataset))\n",
        "print(\"Length of train with HF:\", np.sum([element[-1] for element in train_dataset]))\n",
        "print(\"Length of train with survive patients:\", (len(train_dataset) - np.sum([element[-1] for element in train_dataset])))\n",
        "print(\"Length of val dataset:\", len(val_dataset))\n",
        "print(\"Length of val with HF:\", np.sum([element[-1] for element in val_dataset]))\n",
        "print(\"Length of val with survive patients:\", (len(train_dataset) - np.sum([element[-1] for element in val_dataset])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RNN without additional layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VtHFMPKdX9EP"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_codes, learning_rate = 0.01, hidden_size = 128, dropout = 0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=num_codes, embedding_dim=128)\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=128, hidden_size=hidden_size, batch_first=True, dropout=0)\n",
        "\n",
        "        self.fc = nn.Linear(in_features=hidden_size, out_features=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, masks, rev_x, rev_masks):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = sum_embeddings_with_mask(x, masks)\n",
        "        output, _ = self.rnn(x)\n",
        "        true_h_n = get_last_visit(output, masks)\n",
        "\n",
        "        logits = self.fc(true_h_n)\n",
        "        probs = self.sigmoid(logits)\n",
        "\n",
        "        return probs.view(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RNN with reversed input layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7bVA2ugC3_bk"
      },
      "outputs": [],
      "source": [
        "class RNN_Rev(nn.Module):\n",
        "\n",
        "    def __init__(self, num_codes, learning_rate = 0.01, hidden_size = 128, dropout = 0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=num_codes, embedding_dim=128)\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=128, hidden_size=hidden_size, batch_first=True, dropout=0)\n",
        "\n",
        "        self.rev_rnn = nn.GRU(input_size=128, hidden_size=hidden_size, batch_first=True, dropout=0)\n",
        "\n",
        "        self.fc = nn.Linear(in_features=hidden_size * 2, out_features=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, masks, rev_x, rev_masks):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = sum_embeddings_with_mask(x, masks)\n",
        "        output, _ = self.rnn(x)\n",
        "        true_h_n = get_last_visit(output, masks)\n",
        "\n",
        "        true_h_n_rev = None\n",
        "\n",
        "        rev_x = self.embedding(rev_x)\n",
        "        rev_x = sum_embeddings_with_mask(rev_x, masks)\n",
        "\n",
        "        rev_output, _ = self.rnn(rev_x)\n",
        "        true_h_n_rev = get_last_visit(rev_output, masks)\n",
        "\n",
        "        logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 1))\n",
        "        probs = self.sigmoid(logits)\n",
        "\n",
        "        return probs.view(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RNN with global max-pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "H7QYN5CaQ8BS"
      },
      "outputs": [],
      "source": [
        "class RNN_Pool(nn.Module):\n",
        "\n",
        "    def __init__(self, num_codes, learning_rate = 0.01, hidden_size = 128, dropout = 0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=num_codes, embedding_dim=128)\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=128, hidden_size=hidden_size, batch_first=True, dropout=0)\n",
        "\n",
        "        self.maxpooling = nn.AdaptiveMaxPool1d(hidden_size)  # Add AdaptiveMaxPool1d layer\n",
        "\n",
        "        self.fc = nn.Linear(in_features=hidden_size, out_features=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, masks, rev_x, rev_masks):\n",
        "        # x, masks, y, adj_matrices\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = sum_embeddings_with_mask(x, masks)\n",
        "        output, _ = self.rnn(x)\n",
        "        true_h_n = get_last_visit(output, masks)\n",
        "\n",
        "        x_max = self.maxpooling(true_h_n.unsqueeze(2).permute(0, 2, 1)).squeeze(2)\n",
        "\n",
        "        logits = self.fc(x_max) # for maxpooling layer\n",
        "        probs = self.sigmoid(logits)\n",
        "\n",
        "        return probs.view(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RNN with global max-pooling + reversed input layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JFfhNkLUKyYO"
      },
      "outputs": [],
      "source": [
        "class RNN_Rev_Pool(nn.Module):\n",
        "\n",
        "    def __init__(self, num_codes, learning_rate = 0.01, hidden_size = 128, dropout = 0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=num_codes, embedding_dim=128)\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=128, hidden_size=hidden_size, batch_first=True, dropout=0)\n",
        "        self.rev_rnn = nn.GRU(input_size=128, hidden_size=hidden_size, batch_first=True, dropout=0)\n",
        "\n",
        "        self.maxpooling = nn.AdaptiveMaxPool1d(hidden_size)  # Add AdaptiveMaxPool1d layer\n",
        "\n",
        "        self.fc = nn.Linear(in_features=hidden_size * 2, out_features=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x, masks, rev_x, rev_masks):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = sum_embeddings_with_mask(x, masks)\n",
        "        output, _ = self.rnn(x)\n",
        "        true_h_n = get_last_visit(output, masks)\n",
        "\n",
        "        true_h_n_rev = None\n",
        "        rev_x = self.embedding(rev_x)\n",
        "        rev_x = sum_embeddings_with_mask(rev_x, masks)\n",
        "\n",
        "        rev_output, _ = self.rnn(rev_x)\n",
        "        true_h_n_rev = get_last_visit(rev_output, masks)\n",
        "\n",
        "        true_h_n_max = self.maxpooling(true_h_n.unsqueeze(2).permute(0, 2, 1)).squeeze(2)\n",
        "        true_h_n_rev_max = self.maxpooling(true_h_n_rev.unsqueeze(2).permute(0, 2, 1)).squeeze(2)\n",
        "\n",
        "        logits = self.fc(torch.cat([true_h_n_max, true_h_n_rev_max], 2)) \n",
        "        probs = self.sigmoid(logits)\n",
        "\n",
        "        return probs.view(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fXmSiIEcVn3o"
      },
      "outputs": [],
      "source": [
        "# Loss and Optimizer\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA6J5w0yb49P",
        "outputId": "4e8e95f2-ed42-4b59-9032-30563df9c78c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3yysELMYXip"
      },
      "source": [
        "## Training\n",
        "\n",
        "Hyperparams\n",
        "\n",
        "* \n",
        "\n",
        "Computational requirements\n",
        "\n",
        "*   This code ran success fully on a system with the following specifications:\n",
        "    * Processor: 2.6 GHz Quad-Core intel Core i7\n",
        "    * Memory: 16 GH 2133 MHz LPDDR3\n",
        "    * Gradeon Pro 450 2GB Intel HD Graphics 530 1536MB\n",
        "*   Describe time.... below\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oa9PzOgIVnvl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import *\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "def eval_model(model, val_loader):\n",
        "    model.to(device).eval()\n",
        "    y_pred = torch.LongTensor()\n",
        "    y_score = torch.Tensor()\n",
        "    y_true = torch.LongTensor()\n",
        "    model.eval()\n",
        "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
        "        #x, masks, rev_x, rev_masks, y = x.to(device), masks.to(device), rev_x.to(device), rev_masks.to(device), y.to(device)\n",
        "\n",
        "        y_hat = model(x, masks, rev_x, rev_masks)\n",
        "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
        "        y_hat = (y_hat > 0.5).int()\n",
        "\n",
        "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
        "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
        "\n",
        "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "    roc_auc = roc_auc_score(y_true, y_score)\n",
        "\n",
        "    return p, r, f, roc_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hgrvWizb77Gf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'hidden_size': [64, 128, 256],\n",
        "    'dropout': [0.0, 0.1]\n",
        "}\n",
        "\n",
        "def train_with_hyperparameter_tuning(model, train_loader, val_loader, n_epochs, param_grid):\n",
        "    best_roc_auc_score = 0\n",
        "    best_params = None\n",
        "\n",
        "    # Loop over hyperparameter combinations\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        print(\"Training with hyperparameters:\", params)\n",
        "\n",
        "        # Initialize the model with current hyperparameters\n",
        "        current_model = model(num_codes=len(types), **params).to(device)\n",
        "        optimizer = torch.optim.Adam(current_model.parameters(), lr=params['learning_rate'])\n",
        "        print(f\"current_model: {current_model}\")\n",
        "        # Training loop\n",
        "        for epoch in range(n_epochs):\n",
        "            current_model.train()\n",
        "            train_loss = 0\n",
        "            for x, masks, rev_x, rev_masks, y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                y_hat = current_model(x, masks, rev_x, rev_masks)\n",
        "                y_hat = y_hat.view(y_hat.shape[0])\n",
        "                loss = criterion(y_hat, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            train_loss = train_loss / len(train_loader)\n",
        "            print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        _, _, _, roc_auc = eval_model(current_model, val_loader)\n",
        "        print(\"Validation roc_auc with current hyperparameters:\", roc_auc)\n",
        "\n",
        "        # Check if this set of hyperparameters is the best so far\n",
        "        if roc_auc > best_roc_auc_score:\n",
        "            best_roc_auc_score = roc_auc\n",
        "            best_params = params\n",
        "\n",
        "    print(\"Best hyperparameters:\", best_params)\n",
        "    print(\"Best roc_auc score:\", best_roc_auc_score)\n",
        "\n",
        "    # Train the model with the best hyperparameters on the full training set\n",
        "    best_model = model(num_codes=len(types), **best_params).to(device)\n",
        "    optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
        "    for epoch in range(n_epochs):\n",
        "        best_model.train()\n",
        "        train_loss = 0\n",
        "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = best_model(x, masks, rev_x, rev_masks)\n",
        "            y_hat = y_hat.view(y_hat.shape[0])\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESKidmKYFtnJ",
        "outputId": "3bf48171-2354-4fc6-961f-713165fbbac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.638951\n",
            "Epoch: 2 \t Training Loss: 0.472035\n",
            "Epoch: 3 \t Training Loss: 0.334809\n",
            "Epoch: 4 \t Training Loss: 0.224091\n",
            "Epoch: 5 \t Training Loss: 0.144595\n",
            "Validation roc_auc with current hyperparameters: 0.937493400908035\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.01}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.418942\n",
            "Epoch: 2 \t Training Loss: 0.077670\n",
            "Epoch: 3 \t Training Loss: 0.013355\n",
            "Epoch: 4 \t Training Loss: 0.003987\n",
            "Epoch: 5 \t Training Loss: 0.001999\n",
            "Validation roc_auc with current hyperparameters: 0.9826839826839826\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.1}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.409349\n",
            "Epoch: 2 \t Training Loss: 0.144228\n",
            "Epoch: 3 \t Training Loss: 0.106613\n",
            "Epoch: 4 \t Training Loss: 0.089348\n",
            "Epoch: 5 \t Training Loss: 0.085234\n",
            "Validation roc_auc with current hyperparameters: 0.9581881533101045\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.001}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.573420\n",
            "Epoch: 2 \t Training Loss: 0.321743\n",
            "Epoch: 3 \t Training Loss: 0.172076\n",
            "Epoch: 4 \t Training Loss: 0.091791\n",
            "Epoch: 5 \t Training Loss: 0.054125\n",
            "Validation roc_auc with current hyperparameters: 0.9617780593390349\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.01}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.411707\n",
            "Epoch: 2 \t Training Loss: 0.052889\n",
            "Epoch: 3 \t Training Loss: 0.005099\n",
            "Epoch: 4 \t Training Loss: 0.001488\n",
            "Epoch: 5 \t Training Loss: 0.000874\n",
            "Validation roc_auc with current hyperparameters: 0.9658958927251611\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.1}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.519891\n",
            "Epoch: 2 \t Training Loss: 0.225919\n",
            "Epoch: 3 \t Training Loss: 0.200026\n",
            "Epoch: 4 \t Training Loss: 0.135555\n",
            "Epoch: 5 \t Training Loss: 0.116636\n",
            "Validation roc_auc with current hyperparameters: 0.9548094182240523\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.001}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.493528\n",
            "Epoch: 2 \t Training Loss: 0.201057\n",
            "Epoch: 3 \t Training Loss: 0.087300\n",
            "Epoch: 4 \t Training Loss: 0.044892\n",
            "Epoch: 5 \t Training Loss: 0.025100\n",
            "Validation roc_auc with current hyperparameters: 0.9625171576391088\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.01}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.358449\n",
            "Epoch: 2 \t Training Loss: 0.021501\n",
            "Epoch: 3 \t Training Loss: 0.003717\n",
            "Epoch: 4 \t Training Loss: 0.000801\n",
            "Epoch: 5 \t Training Loss: 0.000479\n",
            "Validation roc_auc with current hyperparameters: 0.9796220040122481\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.1}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.846738\n",
            "Epoch: 2 \t Training Loss: 0.223506\n",
            "Epoch: 3 \t Training Loss: 0.148760\n",
            "Epoch: 4 \t Training Loss: 0.185007\n",
            "Epoch: 5 \t Training Loss: 0.132431\n",
            "Validation roc_auc with current hyperparameters: 0.952908879738148\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.001}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.593849\n",
            "Epoch: 2 \t Training Loss: 0.421636\n",
            "Epoch: 3 \t Training Loss: 0.290154\n",
            "Epoch: 4 \t Training Loss: 0.189002\n",
            "Epoch: 5 \t Training Loss: 0.119902\n",
            "Validation roc_auc with current hyperparameters: 0.953647978038222\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.01}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.361709\n",
            "Epoch: 2 \t Training Loss: 0.049918\n",
            "Epoch: 3 \t Training Loss: 0.009212\n",
            "Epoch: 4 \t Training Loss: 0.002843\n",
            "Epoch: 5 \t Training Loss: 0.001538\n",
            "Validation roc_auc with current hyperparameters: 0.9805722732551999\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.1}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.415382\n",
            "Epoch: 2 \t Training Loss: 0.165088\n",
            "Epoch: 3 \t Training Loss: 0.138602\n",
            "Epoch: 4 \t Training Loss: 0.123217\n",
            "Epoch: 5 \t Training Loss: 0.084369\n",
            "Validation roc_auc with current hyperparameters: 0.9613557174532785\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.001}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.535354\n",
            "Epoch: 2 \t Training Loss: 0.295166\n",
            "Epoch: 3 \t Training Loss: 0.162229\n",
            "Epoch: 4 \t Training Loss: 0.084356\n",
            "Epoch: 5 \t Training Loss: 0.047773\n",
            "Validation roc_auc with current hyperparameters: 0.9604054482103263\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.01}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.380712\n",
            "Epoch: 2 \t Training Loss: 0.041103\n",
            "Epoch: 3 \t Training Loss: 0.003771\n",
            "Epoch: 4 \t Training Loss: 0.001141\n",
            "Epoch: 5 \t Training Loss: 0.000719\n",
            "Validation roc_auc with current hyperparameters: 0.9623059866962306\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.1}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.538073\n",
            "Epoch: 2 \t Training Loss: 0.216853\n",
            "Epoch: 3 \t Training Loss: 0.109719\n",
            "Epoch: 4 \t Training Loss: 0.085305\n",
            "Epoch: 5 \t Training Loss: 0.071594\n",
            "Validation roc_auc with current hyperparameters: 0.9630450849963044\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.001}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.487909\n",
            "Epoch: 2 \t Training Loss: 0.193948\n",
            "Epoch: 3 \t Training Loss: 0.096318\n",
            "Epoch: 4 \t Training Loss: 0.048050\n",
            "Epoch: 5 \t Training Loss: 0.027010\n",
            "Validation roc_auc with current hyperparameters: 0.9401330376940132\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.01}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.364294\n",
            "Epoch: 2 \t Training Loss: 0.025968\n",
            "Epoch: 3 \t Training Loss: 0.002975\n",
            "Epoch: 4 \t Training Loss: 0.001094\n",
            "Epoch: 5 \t Training Loss: 0.000547\n",
            "Validation roc_auc with current hyperparameters: 0.9834230809840566\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.1}\n",
            "current_model: RNN(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.755641\n",
            "Epoch: 2 \t Training Loss: 0.305409\n",
            "Epoch: 3 \t Training Loss: 0.216093\n",
            "Epoch: 4 \t Training Loss: 0.203581\n",
            "Epoch: 5 \t Training Loss: 0.197402\n",
            "Validation roc_auc with current hyperparameters: 0.9510083412522438\n",
            "Best hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.01}\n",
            "Best roc_auc score: 0.9834230809840566\n",
            "Epoch: 1 \t Training Loss: 0.381950\n",
            "Epoch: 2 \t Training Loss: 0.039140\n",
            "Epoch: 3 \t Training Loss: 0.002748\n",
            "Epoch: 4 \t Training Loss: 0.000865\n",
            "Epoch: 5 \t Training Loss: 0.000510\n"
          ]
        }
      ],
      "source": [
        "# Call the training function with hyperparameter tuning - RNN without additional layers\n",
        "best_model_RNN = train_with_hyperparameter_tuning(RNN, train_loader, val_loader, n_epochs=5, param_grid=param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNiMUT7y4ImW",
        "outputId": "675ef519-d38f-49da-9afd-e4275c2d7787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.634741\n",
            "Epoch: 2 \t Training Loss: 0.435689\n",
            "Epoch: 3 \t Training Loss: 0.287276\n",
            "Epoch: 4 \t Training Loss: 0.178957\n",
            "Epoch: 5 \t Training Loss: 0.109499\n",
            "Validation roc_auc with current hyperparameters: 0.9679020166825044\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.362468\n",
            "Epoch: 2 \t Training Loss: 0.041063\n",
            "Epoch: 3 \t Training Loss: 0.006959\n",
            "Epoch: 4 \t Training Loss: 0.001909\n",
            "Epoch: 5 \t Training Loss: 0.001075\n",
            "Validation roc_auc with current hyperparameters: 0.9844789356984479\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.409667\n",
            "Epoch: 2 \t Training Loss: 0.162527\n",
            "Epoch: 3 \t Training Loss: 0.109299\n",
            "Epoch: 4 \t Training Loss: 0.065716\n",
            "Epoch: 5 \t Training Loss: 0.091284\n",
            "Validation roc_auc with current hyperparameters: 0.9594551789673741\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.592458\n",
            "Epoch: 2 \t Training Loss: 0.323264\n",
            "Epoch: 3 \t Training Loss: 0.172238\n",
            "Epoch: 4 \t Training Loss: 0.088644\n",
            "Epoch: 5 \t Training Loss: 0.047480\n",
            "Validation roc_auc with current hyperparameters: 0.9836342519269348\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.333185\n",
            "Epoch: 2 \t Training Loss: 0.029700\n",
            "Epoch: 3 \t Training Loss: 0.004339\n",
            "Epoch: 4 \t Training Loss: 0.001051\n",
            "Epoch: 5 \t Training Loss: 0.000545\n",
            "Validation roc_auc with current hyperparameters: 0.9807834441980783\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.629905\n",
            "Epoch: 2 \t Training Loss: 0.261896\n",
            "Epoch: 3 \t Training Loss: 0.225390\n",
            "Epoch: 4 \t Training Loss: 0.210347\n",
            "Epoch: 5 \t Training Loss: 0.229755\n",
            "Validation roc_auc with current hyperparameters: 0.9648400380107698\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.505680\n",
            "Epoch: 2 \t Training Loss: 0.210338\n",
            "Epoch: 3 \t Training Loss: 0.086132\n",
            "Epoch: 4 \t Training Loss: 0.037147\n",
            "Epoch: 5 \t Training Loss: 0.019478\n",
            "Validation roc_auc with current hyperparameters: 0.9765600253405131\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.342446\n",
            "Epoch: 2 \t Training Loss: 0.050552\n",
            "Epoch: 3 \t Training Loss: 0.006550\n",
            "Epoch: 4 \t Training Loss: 0.000747\n",
            "Epoch: 5 \t Training Loss: 0.000427\n",
            "Validation roc_auc with current hyperparameters: 0.9816281279695913\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 1.158576\n",
            "Epoch: 2 \t Training Loss: 0.240201\n",
            "Epoch: 3 \t Training Loss: 0.308601\n",
            "Epoch: 4 \t Training Loss: 0.278554\n",
            "Epoch: 5 \t Training Loss: 0.318262\n",
            "Validation roc_auc with current hyperparameters: 0.9660014781966001\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.642278\n",
            "Epoch: 2 \t Training Loss: 0.444524\n",
            "Epoch: 3 \t Training Loss: 0.293063\n",
            "Epoch: 4 \t Training Loss: 0.183441\n",
            "Epoch: 5 \t Training Loss: 0.113388\n",
            "Validation roc_auc with current hyperparameters: 0.9389715975081828\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.371762\n",
            "Epoch: 2 \t Training Loss: 0.043308\n",
            "Epoch: 3 \t Training Loss: 0.004976\n",
            "Epoch: 4 \t Training Loss: 0.001767\n",
            "Epoch: 5 \t Training Loss: 0.001055\n",
            "Validation roc_auc with current hyperparameters: 0.9880688417273784\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.488919\n",
            "Epoch: 2 \t Training Loss: 0.145733\n",
            "Epoch: 3 \t Training Loss: 0.120301\n",
            "Epoch: 4 \t Training Loss: 0.087491\n",
            "Epoch: 5 \t Training Loss: 0.088215\n",
            "Validation roc_auc with current hyperparameters: 0.9588216661387393\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.580986\n",
            "Epoch: 2 \t Training Loss: 0.315959\n",
            "Epoch: 3 \t Training Loss: 0.164007\n",
            "Epoch: 4 \t Training Loss: 0.079304\n",
            "Epoch: 5 \t Training Loss: 0.041660\n",
            "Validation roc_auc with current hyperparameters: 0.9668461619681131\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.371674\n",
            "Epoch: 2 \t Training Loss: 0.041991\n",
            "Epoch: 3 \t Training Loss: 0.003042\n",
            "Epoch: 4 \t Training Loss: 0.000969\n",
            "Epoch: 5 \t Training Loss: 0.000598\n",
            "Validation roc_auc with current hyperparameters: 0.9770879526977088\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.611318\n",
            "Epoch: 2 \t Training Loss: 0.279013\n",
            "Epoch: 3 \t Training Loss: 0.187206\n",
            "Epoch: 4 \t Training Loss: 0.124035\n",
            "Epoch: 5 \t Training Loss: 0.141322\n",
            "Validation roc_auc with current hyperparameters: 0.9510083412522438\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.486237\n",
            "Epoch: 2 \t Training Loss: 0.184080\n",
            "Epoch: 3 \t Training Loss: 0.076193\n",
            "Epoch: 4 \t Training Loss: 0.032426\n",
            "Epoch: 5 \t Training Loss: 0.017512\n",
            "Validation roc_auc with current hyperparameters: 0.9567099567099567\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.351798\n",
            "Epoch: 2 \t Training Loss: 0.038668\n",
            "Epoch: 3 \t Training Loss: 0.001444\n",
            "Epoch: 4 \t Training Loss: 0.000569\n",
            "Epoch: 5 \t Training Loss: 0.000331\n",
            "Validation roc_auc with current hyperparameters: 0.9843733502270087\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 1.181566\n",
            "Epoch: 2 \t Training Loss: 0.382827\n",
            "Epoch: 3 \t Training Loss: 0.321905\n",
            "Epoch: 4 \t Training Loss: 0.126514\n",
            "Epoch: 5 \t Training Loss: 0.098565\n",
            "Validation roc_auc with current hyperparameters: 0.9671629183824305\n",
            "Best hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.01}\n",
            "Best roc_auc score: 0.9880688417273784\n",
            "Epoch: 1 \t Training Loss: 0.381963\n",
            "Epoch: 2 \t Training Loss: 0.049635\n",
            "Epoch: 3 \t Training Loss: 0.005284\n",
            "Epoch: 4 \t Training Loss: 0.001733\n",
            "Epoch: 5 \t Training Loss: 0.001089\n"
          ]
        }
      ],
      "source": [
        "# Call the training function with hyperparameter tuning - RNN with a reversed input layer\n",
        "best_model_Rev = train_with_hyperparameter_tuning(RNN_Rev, train_loader, val_loader, n_epochs=5, param_grid=param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QrUPmft4KvA",
        "outputId": "44b78813-e47f-4d61-96ca-4998134e6eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.641524\n",
            "Epoch: 2 \t Training Loss: 0.457636\n",
            "Epoch: 3 \t Training Loss: 0.323237\n",
            "Epoch: 4 \t Training Loss: 0.214128\n",
            "Epoch: 5 \t Training Loss: 0.137468\n",
            "Validation roc_auc with current hyperparameters: 0.9571322985957132\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.01}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.427149\n",
            "Epoch: 2 \t Training Loss: 0.060968\n",
            "Epoch: 3 \t Training Loss: 0.008834\n",
            "Epoch: 4 \t Training Loss: 0.002707\n",
            "Epoch: 5 \t Training Loss: 0.001638\n",
            "Validation roc_auc with current hyperparameters: 0.9761376834547566\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.1}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.397818\n",
            "Epoch: 2 \t Training Loss: 0.135849\n",
            "Epoch: 3 \t Training Loss: 0.104407\n",
            "Epoch: 4 \t Training Loss: 0.099526\n",
            "Epoch: 5 \t Training Loss: 0.061568\n",
            "Validation roc_auc with current hyperparameters: 0.9647344525393305\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.001}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.561984\n",
            "Epoch: 2 \t Training Loss: 0.316462\n",
            "Epoch: 3 \t Training Loss: 0.175344\n",
            "Epoch: 4 \t Training Loss: 0.091100\n",
            "Epoch: 5 \t Training Loss: 0.053519\n",
            "Validation roc_auc with current hyperparameters: 0.9564987857670785\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.01}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.366437\n",
            "Epoch: 2 \t Training Loss: 0.042117\n",
            "Epoch: 3 \t Training Loss: 0.004805\n",
            "Epoch: 4 \t Training Loss: 0.001608\n",
            "Epoch: 5 \t Training Loss: 0.000897\n",
            "Validation roc_auc with current hyperparameters: 0.9785661492978566\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.1}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.506820\n",
            "Epoch: 2 \t Training Loss: 0.177427\n",
            "Epoch: 3 \t Training Loss: 0.105138\n",
            "Epoch: 4 \t Training Loss: 0.044157\n",
            "Epoch: 5 \t Training Loss: 0.034674\n",
            "Validation roc_auc with current hyperparameters: 0.9778270509977827\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.001}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.496319\n",
            "Epoch: 2 \t Training Loss: 0.212336\n",
            "Epoch: 3 \t Training Loss: 0.099202\n",
            "Epoch: 4 \t Training Loss: 0.047365\n",
            "Epoch: 5 \t Training Loss: 0.026224\n",
            "Validation roc_auc with current hyperparameters: 0.9696969696969696\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.01}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.402118\n",
            "Epoch: 2 \t Training Loss: 0.049298\n",
            "Epoch: 3 \t Training Loss: 0.003584\n",
            "Epoch: 4 \t Training Loss: 0.001046\n",
            "Epoch: 5 \t Training Loss: 0.000545\n",
            "Validation roc_auc with current hyperparameters: 0.9720198500686306\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.1}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.625582\n",
            "Epoch: 2 \t Training Loss: 0.171027\n",
            "Epoch: 3 \t Training Loss: 0.138469\n",
            "Epoch: 4 \t Training Loss: 0.160146\n",
            "Epoch: 5 \t Training Loss: 0.192253\n",
            "Validation roc_auc with current hyperparameters: 0.9615668883961567\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.001}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.624498\n",
            "Epoch: 2 \t Training Loss: 0.450904\n",
            "Epoch: 3 \t Training Loss: 0.317204\n",
            "Epoch: 4 \t Training Loss: 0.214000\n",
            "Epoch: 5 \t Training Loss: 0.142049\n",
            "Validation roc_auc with current hyperparameters: 0.9453067257945307\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.01}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.402365\n",
            "Epoch: 2 \t Training Loss: 0.057442\n",
            "Epoch: 3 \t Training Loss: 0.011322\n",
            "Epoch: 4 \t Training Loss: 0.003455\n",
            "Epoch: 5 \t Training Loss: 0.001685\n",
            "Validation roc_auc with current hyperparameters: 0.9660014781966\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.1}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.428234\n",
            "Epoch: 2 \t Training Loss: 0.149236\n",
            "Epoch: 3 \t Training Loss: 0.122690\n",
            "Epoch: 4 \t Training Loss: 0.110697\n",
            "Epoch: 5 \t Training Loss: 0.118117\n",
            "Validation roc_auc with current hyperparameters: 0.9742371449688523\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.001}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.570847\n",
            "Epoch: 2 \t Training Loss: 0.324610\n",
            "Epoch: 3 \t Training Loss: 0.181004\n",
            "Epoch: 4 \t Training Loss: 0.098884\n",
            "Epoch: 5 \t Training Loss: 0.058051\n",
            "Validation roc_auc with current hyperparameters: 0.9633618414106218\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.01}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.338847\n",
            "Epoch: 2 \t Training Loss: 0.034457\n",
            "Epoch: 3 \t Training Loss: 0.004352\n",
            "Epoch: 4 \t Training Loss: 0.001430\n",
            "Epoch: 5 \t Training Loss: 0.000801\n",
            "Validation roc_auc with current hyperparameters: 0.9849012775842043\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.1}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.478726\n",
            "Epoch: 2 \t Training Loss: 0.151178\n",
            "Epoch: 3 \t Training Loss: 0.134158\n",
            "Epoch: 4 \t Training Loss: 0.132422\n",
            "Epoch: 5 \t Training Loss: 0.095071\n",
            "Validation roc_auc with current hyperparameters: 0.9695913842255306\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.001}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.502678\n",
            "Epoch: 2 \t Training Loss: 0.225652\n",
            "Epoch: 3 \t Training Loss: 0.098630\n",
            "Epoch: 4 \t Training Loss: 0.050468\n",
            "Epoch: 5 \t Training Loss: 0.028887\n",
            "Validation roc_auc with current hyperparameters: 0.9666349910252349\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.01}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.357550\n",
            "Epoch: 2 \t Training Loss: 0.041249\n",
            "Epoch: 3 \t Training Loss: 0.005110\n",
            "Epoch: 4 \t Training Loss: 0.001166\n",
            "Epoch: 5 \t Training Loss: 0.000601\n",
            "Validation roc_auc with current hyperparameters: 0.9758209270404393\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.1}\n",
            "current_model: RNN_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.831176\n",
            "Epoch: 2 \t Training Loss: 0.252682\n",
            "Epoch: 3 \t Training Loss: 0.206657\n",
            "Epoch: 4 \t Training Loss: 0.166434\n",
            "Epoch: 5 \t Training Loss: 0.128436\n",
            "Validation roc_auc with current hyperparameters: 0.9668461619681132\n",
            "Best hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.01}\n",
            "Best roc_auc score: 0.9849012775842043\n",
            "Epoch: 1 \t Training Loss: 0.378489\n",
            "Epoch: 2 \t Training Loss: 0.041521\n",
            "Epoch: 3 \t Training Loss: 0.003983\n",
            "Epoch: 4 \t Training Loss: 0.001414\n",
            "Epoch: 5 \t Training Loss: 0.000885\n"
          ]
        }
      ],
      "source": [
        "# Call the training function with hyperparameter tuning - RNN with global max-pooling\n",
        "best_model_pool = train_with_hyperparameter_tuning(RNN_Pool, train_loader, val_loader, n_epochs=5, param_grid=param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p7ZlNNydEFF",
        "outputId": "af130456-9f2f-47cc-c592-7cd411c075c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.615994\n",
            "Epoch: 2 \t Training Loss: 0.410773\n",
            "Epoch: 3 \t Training Loss: 0.256110\n",
            "Epoch: 4 \t Training Loss: 0.149673\n",
            "Epoch: 5 \t Training Loss: 0.088141\n",
            "Validation roc_auc with current hyperparameters: 0.9747650723260479\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.400939\n",
            "Epoch: 2 \t Training Loss: 0.049918\n",
            "Epoch: 3 \t Training Loss: 0.006326\n",
            "Epoch: 4 \t Training Loss: 0.002145\n",
            "Epoch: 5 \t Training Loss: 0.001130\n",
            "Validation roc_auc with current hyperparameters: 0.976348854397635\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.501717\n",
            "Epoch: 2 \t Training Loss: 0.186780\n",
            "Epoch: 3 \t Training Loss: 0.118024\n",
            "Epoch: 4 \t Training Loss: 0.129472\n",
            "Epoch: 5 \t Training Loss: 0.084456\n",
            "Validation roc_auc with current hyperparameters: 0.9647344525393304\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.566877\n",
            "Epoch: 2 \t Training Loss: 0.294077\n",
            "Epoch: 3 \t Training Loss: 0.147061\n",
            "Epoch: 4 \t Training Loss: 0.071798\n",
            "Epoch: 5 \t Training Loss: 0.039030\n",
            "Validation roc_auc with current hyperparameters: 0.9741315594974131\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.373232\n",
            "Epoch: 2 \t Training Loss: 0.041322\n",
            "Epoch: 3 \t Training Loss: 0.003857\n",
            "Epoch: 4 \t Training Loss: 0.001356\n",
            "Epoch: 5 \t Training Loss: 0.000727\n",
            "Validation roc_auc with current hyperparameters: 0.9869074015415478\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.819770\n",
            "Epoch: 2 \t Training Loss: 0.238705\n",
            "Epoch: 3 \t Training Loss: 0.116725\n",
            "Epoch: 4 \t Training Loss: 0.092913\n",
            "Epoch: 5 \t Training Loss: 0.061338\n",
            "Validation roc_auc with current hyperparameters: 0.9606166191532044\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.553905\n",
            "Epoch: 2 \t Training Loss: 0.241855\n",
            "Epoch: 3 \t Training Loss: 0.094901\n",
            "Epoch: 4 \t Training Loss: 0.040200\n",
            "Epoch: 5 \t Training Loss: 0.020790\n",
            "Validation roc_auc with current hyperparameters: 0.9693802132826522\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.345995\n",
            "Epoch: 2 \t Training Loss: 0.028661\n",
            "Epoch: 3 \t Training Loss: 0.004982\n",
            "Epoch: 4 \t Training Loss: 0.000449\n",
            "Epoch: 5 \t Training Loss: 0.000217\n",
            "Validation roc_auc with current hyperparameters: 0.9860627177700348\n",
            "Training with hyperparameters: {'dropout': 0.0, 'hidden_size': 256, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 1.222766\n",
            "Epoch: 2 \t Training Loss: 0.459556\n",
            "Epoch: 3 \t Training Loss: 0.554911\n",
            "Epoch: 4 \t Training Loss: 0.759253\n",
            "Epoch: 5 \t Training Loss: 0.495843\n",
            "Validation roc_auc with current hyperparameters: 0.9596135571745328\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.659054\n",
            "Epoch: 2 \t Training Loss: 0.459547\n",
            "Epoch: 3 \t Training Loss: 0.297630\n",
            "Epoch: 4 \t Training Loss: 0.178959\n",
            "Epoch: 5 \t Training Loss: 0.106964\n",
            "Validation roc_auc with current hyperparameters: 0.965895892725161\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.377807\n",
            "Epoch: 2 \t Training Loss: 0.051374\n",
            "Epoch: 3 \t Training Loss: 0.006763\n",
            "Epoch: 4 \t Training Loss: 0.001818\n",
            "Epoch: 5 \t Training Loss: 0.001026\n",
            "Validation roc_auc with current hyperparameters: 0.9864850596557914\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 64, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 64, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 64, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=64)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.436498\n",
            "Epoch: 2 \t Training Loss: 0.181667\n",
            "Epoch: 3 \t Training Loss: 0.115082\n",
            "Epoch: 4 \t Training Loss: 0.126264\n",
            "Epoch: 5 \t Training Loss: 0.123703\n",
            "Validation roc_auc with current hyperparameters: 0.9622004012247914\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.595411\n",
            "Epoch: 2 \t Training Loss: 0.327170\n",
            "Epoch: 3 \t Training Loss: 0.169835\n",
            "Epoch: 4 \t Training Loss: 0.083837\n",
            "Epoch: 5 \t Training Loss: 0.042223\n",
            "Validation roc_auc with current hyperparameters: 0.9625171576391088\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.356854\n",
            "Epoch: 2 \t Training Loss: 0.038060\n",
            "Epoch: 3 \t Training Loss: 0.004319\n",
            "Epoch: 4 \t Training Loss: 0.001074\n",
            "Epoch: 5 \t Training Loss: 0.000616\n",
            "Validation roc_auc with current hyperparameters: 0.978882905712174\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 128, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 128, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=128)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.624883\n",
            "Epoch: 2 \t Training Loss: 0.208482\n",
            "Epoch: 3 \t Training Loss: 0.098339\n",
            "Epoch: 4 \t Training Loss: 0.179608\n",
            "Epoch: 5 \t Training Loss: 0.158639\n",
            "Validation roc_auc with current hyperparameters: 0.9535951853025024\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.001}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.524251\n",
            "Epoch: 2 \t Training Loss: 0.218288\n",
            "Epoch: 3 \t Training Loss: 0.089014\n",
            "Epoch: 4 \t Training Loss: 0.039169\n",
            "Epoch: 5 \t Training Loss: 0.020291\n",
            "Validation roc_auc with current hyperparameters: 0.9726533628972652\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.01}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 0.410684\n",
            "Epoch: 2 \t Training Loss: 0.047936\n",
            "Epoch: 3 \t Training Loss: 0.006812\n",
            "Epoch: 4 \t Training Loss: 0.001426\n",
            "Epoch: 5 \t Training Loss: 0.000405\n",
            "Validation roc_auc with current hyperparameters: 0.9786717347692957\n",
            "Training with hyperparameters: {'dropout': 0.1, 'hidden_size': 256, 'learning_rate': 0.1}\n",
            "current_model: RNN_Rev_Pool(\n",
            "  (embedding): Embedding(7328, 128)\n",
            "  (rnn): GRU(128, 256, batch_first=True)\n",
            "  (rev_rnn): GRU(128, 256, batch_first=True)\n",
            "  (maxpooling): AdaptiveMaxPool1d(output_size=256)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch: 1 \t Training Loss: 1.384449\n",
            "Epoch: 2 \t Training Loss: 0.285415\n",
            "Epoch: 3 \t Training Loss: 0.240074\n",
            "Epoch: 4 \t Training Loss: 0.436310\n",
            "Epoch: 5 \t Training Loss: 0.395412\n",
            "Validation roc_auc with current hyperparameters: 0.9560236511456024\n",
            "Best hyperparameters: {'dropout': 0.0, 'hidden_size': 128, 'learning_rate': 0.01}\n",
            "Best roc_auc score: 0.9869074015415478\n",
            "Epoch: 1 \t Training Loss: 0.338690\n",
            "Epoch: 2 \t Training Loss: 0.031729\n",
            "Epoch: 3 \t Training Loss: 0.002916\n",
            "Epoch: 4 \t Training Loss: 0.000921\n",
            "Epoch: 5 \t Training Loss: 0.000569\n"
          ]
        }
      ],
      "source": [
        "# Call the training function with hyperparameter  - RNN with global max-pooling + reversed input layer\n",
        "best_model_rev_pool = train_with_hyperparameter_tuning(RNN_Rev_Pool, train_loader, val_loader, n_epochs=5, param_grid=param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6IkqrW3c9jl"
      },
      "source": [
        "Evaluation\n",
        "\n",
        "* RNN without additional layers\n",
        "* RNN with a reversed input layer\n",
        "* RNN with global max-pooling\n",
        "* RNN with global max-pooling + reversed input layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SV_RGvoc813",
        "outputId": "d05a2cb5-6538-438e-cb0e-6b91f28ac684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.9474\n",
            "Recall: 0.9351\n",
            "F1-score: 0.9412\n",
            "ROC AUC score: 0.9692\n"
          ]
        }
      ],
      "source": [
        "# RNN without additional layers\n",
        "p, r, f, roc_auc  = eval_model(best_model_RNN, val_loader)\n",
        "print(f'Precision: {p:.4f}')\n",
        "print(f'Recall: {r:.4f}')\n",
        "print(f'F1-score: {f:.4f}')\n",
        "print(f'ROC AUC score: {roc_auc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx-EJvEDc8uT",
        "outputId": "e035a5aa-473c-4efa-d73d-6ff5516c955f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.9494\n",
            "Recall: 0.9740\n",
            "F1-score: 0.9615\n",
            "ROC AUC score: 0.9860\n"
          ]
        }
      ],
      "source": [
        "# RNN with a reversed input layer\n",
        "p, r, f, roc_auc  = eval_model(best_model_Rev, val_loader)\n",
        "print(f'Precision: {p:.4f}')\n",
        "print(f'Recall: {r:.4f}')\n",
        "print(f'F1-score: {f:.4f}')\n",
        "print(f'ROC AUC score: {roc_auc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN0b-tedc8lT",
        "outputId": "87a3da27-c711-4223-d501-d90a2022640a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.9375\n",
            "Recall: 0.9740\n",
            "F1-score: 0.9554\n",
            "ROC AUC score: 0.9758\n"
          ]
        }
      ],
      "source": [
        "# RNN with global max-pooling\n",
        "p, r, f, roc_auc  = eval_model(best_model_pool, val_loader)\n",
        "print(f'Precision: {p:.4f}')\n",
        "print(f'Recall: {r:.4f}')\n",
        "print(f'F1-score: {f:.4f}')\n",
        "print(f'ROC AUC score: {roc_auc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZqQA7Y2dNfR",
        "outputId": "4e01a84f-7911-4c1b-bb87-6fb080c2866b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.9351\n",
            "Recall: 0.9351\n",
            "F1-score: 0.9351\n",
            "ROC AUC score: 0.9844\n"
          ]
        }
      ],
      "source": [
        "# RNN with global max-pooling + reversed input layer\n",
        "p, r, f, roc_auc  = eval_model(best_model_rev_pool, val_loader)\n",
        "print(f'Precision: {p:.4f}')\n",
        "print(f'Recall: {r:.4f}')\n",
        "print(f'F1-score: {f:.4f}')\n",
        "print(f'ROC AUC score: {roc_auc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g1flD91dcKO"
      },
      "source": [
        "### Training/Testing the Machine Learning Model: Random Forest (RF), Logistic Regression(LR), Support Vector Machine (SVM)\n",
        "\n",
        "Based on the paper, they computed the counts of each medical event for each patient to predict the heart failure outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "IJ8eFjM7dTLS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "X = grouped_df[['subject_id','seq_num']]\n",
        "y = grouped_df['hfs']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfGIOasXdlfS"
      },
      "source": [
        "Random Forest (RF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzs-qUxndTFj",
        "outputId": "d522e37d-22fb-46df-cbcc-719770f183c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "Precision: 0.2667\n",
            "Recall: 0.1905\n",
            "F1-score: 0.2222\n",
            "ROC AUC score: 0.7849\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=16), param_grid, cv=5, scoring='roc_auc')\n",
        "\n",
        "# Perform the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "# Fit the model with best parameters\n",
        "best_rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the best model\n",
        "y_pred = best_rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "roc_auc = roc_auc_score(y_test, best_rf_classifier.predict_proba(X_test)[:, 1])\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-score: {fscore:.4f}')\n",
        "print(f'ROC AUC score: {roc_auc:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeBUaqUgdwom"
      },
      "source": [
        "Logistic Regression (LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PlDp3JRdS4Z",
        "outputId": "45de89a7-684d-43e0-dd4a-b4fd8cca406d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Precision: 0.4000\n",
            "Recall: 0.0952\n",
            "F1-score: 0.1538\n",
            "ROC AUC score: 0.8500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/Users/bk/CS598_DLH_Final_Project/env/lib/python3.11/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(LogisticRegression(random_state=16, max_iter=100), param_grid, cv=5, scoring='roc_auc')\n",
        "\n",
        "# Perform the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_logreg = grid_search.best_estimator_\n",
        "\n",
        "# Fit the model with best parameters\n",
        "best_logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the best model\n",
        "y_pred = best_logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "roc_auc = roc_auc_score(y_test, best_logreg.predict_proba(X_test)[:, 1])\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-score: {fscore:.4f}')\n",
        "print(f'ROC AUC score: {roc_auc:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwlFuLLdeCVe"
      },
      "source": [
        "Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDDMK8XNd1Yf",
        "outputId": "5a7d576e-dc7b-4b75-e29b-20108c0e8a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.2500\n",
            "Recall: 0.0476\n",
            "F1-score: 0.0800\n",
            "ROC AUC score: 0.7765\n"
          ]
        }
      ],
      "source": [
        "svm_classifier = SVC(kernel='linear', C=1, probability=True, random_state=16)\n",
        "\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_prob = svm_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "threshold = 0.5  # Adjust this threshold based on the precision-recall trade-off\n",
        "y_pred = (y_prob > threshold).astype(int)\n",
        "\n",
        "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "roc_auc = roc_auc_score(y_test, svm_classifier.predict_proba(X_test)[:, 1])\n",
        "\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-score: {fscore:.4f}')\n",
        "print(f'ROC AUC score: {roc_auc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training/Testing the Deep Learning Model: Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM), RETAIN\n",
        "\n",
        "\n",
        "Disclaimer: In the development of the Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM), and RETAIN models, we leveraged the foundational structure code provided in Homework 3-4 as a starting point. While our implementations may share similarities with the base code, significant modifications and optimizations have been made to adapt them to our specific research objectives and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert len(pids) == len(vids) == len(hfs) == len(seqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of heart failure patients: 375\n",
            "ratio of heart failure patients: 0.38\n"
          ]
        }
      ],
      "source": [
        "print(\"number of heart failure patients:\", sum(hfs))\n",
        "print(\"ratio of heart failure patients: %.2f\" % (sum(hfs) / len(hfs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "    sequences, labels = zip(*data)\n",
        "\n",
        "    y = torch.tensor(labels, dtype=torch.float)\n",
        "    \n",
        "    num_patients = len(sequences)\n",
        "    num_visits = [len(patient) for patient in sequences]\n",
        "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
        "\n",
        "    max_num_visits = max(num_visits)\n",
        "    max_num_codes = max(num_codes)\n",
        "    \n",
        "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "    for i_patient, patient in enumerate(sequences):\n",
        "        for j_visit, visit in enumerate(patient):\n",
        "            for k_diag, diag in enumerate(visit):\n",
        "                x[i_patient][j_visit][k_diag] = diag\n",
        "                masks[i_patient][j_visit][k_diag] = 1\n",
        "    \n",
        "    return x, masks, y\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "def eval_model(model, val_loader):\n",
        "    model.eval()\n",
        "    y_pred = torch.LongTensor()\n",
        "    y_score = torch.Tensor()\n",
        "    y_true = torch.LongTensor()\n",
        "    model.eval()\n",
        "    for x, masks, y in val_loader:\n",
        "        y_hat = model(x, masks)\n",
        "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
        "        y_hat = (y_hat > 0.5).int()\n",
        "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
        "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
        "    \n",
        "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "    \n",
        "    roc_auc = roc_auc_score(y_true, y_score)\n",
        "    \n",
        "    return p, r, f, roc_auc\n",
        "\n",
        "def train(model, train_loader, val_loader, n_epochs, optimizer):\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x, masks, y in train_loader:\n",
        "            loss = None\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(x, masks)\n",
        "            y_hat = y_hat.view(y_hat.shape[0])\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "        p, r, f, roc_auc = eval_model(model, val_loader)\n",
        "        print('Epoch: {} \\t Validation p: {:.3f}, r:{:.3f}, f: {:.3f}, roc_auc: {:.3f}'\n",
        "              .format(epoch+1, p, r, f, roc_auc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gated Recurrent Unit (GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NaiveGRU(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_codes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=num_codes, embedding_dim=128)\n",
        "        self.gru = nn.GRU(input_size=128, hidden_size=128, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=128, out_features=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x, masks):\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.embedding(x)\n",
        "        x = sum_embeddings_with_mask(x, masks)\n",
        "        output, _ = self.gru(x)\n",
        "        true_h_n = get_last_visit(output, masks)\n",
        "        logits = self.fc(true_h_n)\n",
        "        probs = self.sigmoid(logits)\n",
        "        return probs.view(batch_size)\n",
        "\n",
        "naive_gru = NaiveGRU(num_codes = len(types))\n",
        "\n",
        "gru_optimizer = torch.optim.Adam(naive_gru.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \t Training Loss: 0.576874\n",
            "Epoch: 1 \t Validation p: 0.793, r:0.844, f: 0.818, roc_auc: 0.922\n",
            "Epoch: 2 \t Training Loss: 0.319999\n",
            "Epoch: 2 \t Validation p: 0.833, r:0.909, f: 0.870, roc_auc: 0.952\n",
            "Epoch: 3 \t Training Loss: 0.173464\n",
            "Epoch: 3 \t Validation p: 0.885, r:0.896, f: 0.890, roc_auc: 0.959\n",
            "Epoch: 4 \t Training Loss: 0.088321\n",
            "Epoch: 4 \t Validation p: 0.911, r:0.935, f: 0.923, roc_auc: 0.964\n",
            "Epoch: 5 \t Training Loss: 0.048066\n",
            "Epoch: 5 \t Validation p: 0.910, r:0.922, f: 0.916, roc_auc: 0.965\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 5   # number of epochs to train the model\n",
        "\n",
        "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)\n",
        "train(naive_gru, train_loader, val_loader, n_epochs, gru_optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Long Short-Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NaiveLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_codes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=num_codes, embedding_dim=128)\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=128, out_features=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x, masks):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.embedding(x)\n",
        "        x = sum_embeddings_with_mask(x, masks)\n",
        "\n",
        "        output, _ = self.lstm(x)\n",
        "        true_h_n = get_last_visit(output, masks)\n",
        "\n",
        "        logits = self.fc(true_h_n)\n",
        "        probs = self.sigmoid(logits)\n",
        "        return probs.view(batch_size)\n",
        "\n",
        "naive_lstm = NaiveLSTM(num_codes = len(types))\n",
        "\n",
        "lstm_optimizer = torch.optim.Adam(naive_lstm.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \t Training Loss: 0.619597\n",
            "Epoch: 1 \t Validation p: 0.731, r:0.987, f: 0.840, roc_auc: 0.959\n",
            "Epoch: 2 \t Training Loss: 0.403011\n",
            "Epoch: 2 \t Validation p: 0.807, r:0.922, f: 0.861, roc_auc: 0.956\n",
            "Epoch: 3 \t Training Loss: 0.235102\n",
            "Epoch: 3 \t Validation p: 0.833, r:0.974, f: 0.898, roc_auc: 0.961\n",
            "Epoch: 4 \t Training Loss: 0.125985\n",
            "Epoch: 4 \t Validation p: 0.877, r:0.922, f: 0.899, roc_auc: 0.959\n",
            "Epoch: 5 \t Training Loss: 0.071157\n",
            "Epoch: 5 \t Validation p: 0.887, r:0.922, f: 0.904, roc_auc: 0.960\n"
          ]
        }
      ],
      "source": [
        "train(naive_lstm, train_loader, val_loader, n_epochs, lstm_optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RETAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "    sequences, labels = zip(*data)\n",
        "\n",
        "    y = torch.tensor(labels, dtype=torch.float)\n",
        "    \n",
        "    num_patients = len(sequences)\n",
        "    num_visits = [len(patient) for patient in sequences]\n",
        "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
        "\n",
        "    max_num_visits = max(num_visits)\n",
        "    max_num_codes = max(num_codes)\n",
        "    \n",
        "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "    for i_patient, patient in enumerate(sequences):\n",
        "        for j_visit, visit in enumerate(patient):\n",
        "            for k_diag, diag in enumerate(visit):\n",
        "                x[i_patient][j_visit][k_diag] = diag\n",
        "                masks[i_patient][j_visit][k_diag] = 1\n",
        "        \n",
        "        for j_visit, visit in enumerate(patient):\n",
        "            rev_x[i_patient][j_visit] = x[i_patient][len(patient) - 1 - j_visit]\n",
        "            rev_masks[i_patient][j_visit] = masks[i_patient][len(patient) - 1 - j_visit]\n",
        "    \n",
        "    return x, masks, rev_x, rev_masks, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(train_dataset, val_dataset, collate_fn):\n",
        "    batch_size = 32\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "    \n",
        "    return train_loader, val_loader\n",
        "\n",
        "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AlphaAttention(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.a_att = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, g, rev_masks):\n",
        "        g = self.a_att(g)\n",
        "        g[torch.sum(rev_masks, dim = 2) == 0] = -1e9\n",
        "        m = nn.Softmax(dim=1)\n",
        "        return m(g)\n",
        "    \n",
        "class BetaAttention(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.b_att = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, h):\n",
        "        h = self.b_att(h)\n",
        "        return torch.tanh(h)\n",
        "    \n",
        "def attention_sum(alpha, beta, rev_v, rev_masks):\n",
        "    masks_ = torch.sum(rev_masks, dim = 2)\n",
        "    masks_[masks_ != 0] = True\n",
        "    masks_ = masks_.unsqueeze(2)\n",
        "\n",
        "    return torch.sum(alpha * beta * rev_v * masks_, dim = 1)\n",
        "\n",
        "def sum_embeddings_with_mask(x, masks):\n",
        "    x = x * masks.unsqueeze(-1)\n",
        "    x = torch.sum(x, dim = -2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RETAIN(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_codes, embedding_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_codes, embedding_dim)\n",
        "        self.rnn_a = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
        "        self.rnn_b = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
        "        self.att_a = AlphaAttention(embedding_dim)\n",
        "        self.att_b = BetaAttention(embedding_dim)\n",
        "        self.fc = nn.Linear(embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x, masks, rev_x, rev_masks):\n",
        "        rev_x = self.embedding(rev_x)\n",
        "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
        "        g, _ = self.rnn_a(rev_x)\n",
        "        h, _ = self.rnn_b(rev_x)\n",
        "        alpha = self.att_a(g, rev_masks)\n",
        "        beta = self.att_b(h)\n",
        "        c = attention_sum(alpha, beta, rev_x, rev_masks)\n",
        "        logits = self.fc(c)\n",
        "        probs = self.sigmoid(logits)\n",
        "        return probs.squeeze(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval(model, val_loader):\n",
        "    model.eval()\n",
        "    y_pred = torch.LongTensor()\n",
        "    y_score = torch.Tensor()\n",
        "    y_true = torch.LongTensor()\n",
        "    model.eval()\n",
        "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
        "        y_logit = model(x, masks, rev_x, rev_masks)\n",
        "        y_hat = model(x, masks, rev_x, rev_masks)\n",
        "        y_hat = (y_hat > 0.5).int()\n",
        "        y_score = torch.cat((y_score,  y_logit.detach().to('cpu')), dim=0)\n",
        "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
        "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
        "    \n",
        "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "    roc_auc = roc_auc_score(y_true, y_score)\n",
        "    return p, r, f, roc_auc\n",
        "    \n",
        "def train(model, train_loader, val_loader, n_epochs):\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(x, masks, rev_x, rev_masks)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "        p, r, f, roc_auc = eval(model, val_loader)\n",
        "        print('Epoch: {} \\t Validation p: {:.3f}, r:{:.3f}, f: {:.3f}, roc_auc: {:.3f}'.format(epoch+1, p, r, f, roc_auc))\n",
        "    return round(roc_auc, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \t Training Loss: 0.588016\n",
            "Epoch: 1 \t Validation p: 0.643, r:0.935, f: 0.762, roc_auc: 0.936\n",
            "Epoch: 2 \t Training Loss: 0.263385\n",
            "Epoch: 2 \t Validation p: 0.880, r:0.857, f: 0.868, roc_auc: 0.957\n",
            "Epoch: 3 \t Training Loss: 0.122212\n",
            "Epoch: 3 \t Validation p: 0.934, r:0.922, f: 0.928, roc_auc: 0.967\n",
            "Epoch: 4 \t Training Loss: 0.061360\n",
            "Epoch: 4 \t Validation p: 0.945, r:0.896, f: 0.920, roc_auc: 0.965\n",
            "Epoch: 5 \t Training Loss: 0.037007\n",
            "Epoch: 5 \t Validation p: 0.958, r:0.896, f: 0.926, roc_auc: 0.966\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.97"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain = RETAIN(num_codes = len(types))\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(retain.parameters(), lr=1e-3)\n",
        "\n",
        "n_epochs = 5\n",
        "train(retain, train_loader, val_loader, n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeXMAFs2eJZr"
      },
      "source": [
        "# Results\n",
        "\n",
        "In conclusion, the RNN model outperformed all other machine learning models in terms of Precision, Recall, F1-score, and ROC AUC score, demonstrating its superior predictive performance for the task of predicting patient risk. The Logistic Regression, Random Forest, and SVM models showed varying degrees of performance, with the SVM model suffering from severe class imbalance issues\n",
        "\n",
        "\n",
        "* Table of results (no need to include additional experiments, but main reproducibility result should be included)\n",
        "\n",
        "| Model        | Precision  | Recall     | F1-score   | ROC AUC    |\n",
        "| ------------ | ---------- | ---------- | ---------- | ---------- |\n",
        "| RF           | 0.2667     | 0.1905     | 0.2222     | 0.7849     |\n",
        "| LR           | 0.4000     | 0.0952     | 0.1538     | 0.8500     |\n",
        "| SVM          | 0.2500     | 0.0476     | 0.0800     | 0.7765     |\n",
        "| GRU          | 0.9100     | 0.9220     | 0.9160     | 0.9650     |\n",
        "| LSTM         | 0.8870     | 0.9220     | 0.9040     | 0.9600     |\n",
        "| RETAIN       | 0.9580     | 0.8960     | 0.9260     | 0.9660     |\n",
        "| RNN          | 0.9474     | 0.9351     | 0.9412     | 0.9692     |\n",
        "| RNN+rev      | 0.9494     | 0.9740     | 0.9615     | 0.9860     |\n",
        "| RNN+max_pool | 0.9375     | 0.9740     | 0.9554     | 0.9758     |\n",
        "| RNN+rev      | 0.9351     | 0.9351     | 0.9351     | 0.9844     |\n",
        "|  + max_pool  |            |            |            |            |\n",
        "\n",
        "\n",
        "* All claims should be supported by experiment results\n",
        "    * r\n",
        "\n",
        "\n",
        "* Discuss with respect to the hypothesis and results from the original paper\n",
        "    * r\n",
        "\n",
        "\n",
        "* Experiments beyond the original paper Each experiment should include results and a discussion\n",
        "    * r\n",
        "\n",
        "\n",
        "* Ablation Study.\n",
        "    * r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57ftBkfgeRsK"
      },
      "source": [
        "# Discussion\n",
        "Implications of the experimental results, whether the original paper was reproducible, and if it wasn’t, what factors made it irreproducible\n",
        "\n",
        "“What was easy”\n",
        "\n",
        "“What was difficult”\n",
        "\n",
        "Recommendations to the original authors or others who work in this area for improving reproducibility\n",
        "\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "    * The original paper's code is not fully reproducible due to the lack of necessary files and resources. Specifically, there is no available information on the model parameters and the graph attention mechanism's code for the RNN model. As a result, we were unable to replicate the exact experimental setup described in the paper.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "    * The lack of parameter information and the absence of the graph attention mechanism's code for the RNN model in the original paper's GitHub repository make it impossible to reproduce the experiments accurately. Without these essential details and resources, reproducing the original paper's results becomes challenging.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "    * Understanding the high-level concepts and methodologies presented in the paper was relatively straightforward. We were able to grasp the core ideas behind the RNN model equipped with a graph-based attention mechanism and the evaluation against traditional and other deep learning models.\n",
        "    * The most challenging aspect of the reproduction was the lack of detailed information and resources required to implement the exact experimental setup described in the paper. Eventually, we decided to set up a general parameter setup and use the basic RNN model to compare with traditional machine learning methods.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "    * We suggest providing comprehensive documentation and code repositories that include all necessary files, parameter information, and model implementations to facilitate reproducibility. Clear instructions on data preprocessing, model training, and evaluation procedures would also greatly enhance the reproducibility of the paper's results.\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZMg8kZMe-0E"
      },
      "source": [
        "# Public Github Repo\n",
        "\n",
        "https://github.com/BK1147/CS598_DLH_Final_Project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1.   C. Yin, R. Zhao, B. Qian, X. Lv and P. Zhang, \"Domain Knowledge Guided Deep Learning with Electronic Health Records,\" 2019 IEEE International Conference on Data Mining (ICDM), Beijing, China, 2019, pp. 738-747, doi: 10.1109/ICDM.2019.00084.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.9 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "cb99a5fe2f7a378128bb85ee9415e6e72b581271970b7af8eaf62918633a7a23"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
